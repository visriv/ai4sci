
# ğŸ§  ai4sci - reasoning and interpretation for science
*A Modern Agentic AI + LLMOps Project *

---

## ğŸŒ Project Overview  
ai4sci is a **production-grade LLM Agent** designed to perform **agentic reasoning and interpretation for scientific discovery**.
Before jumping into research, we experiemnt and implement the tools used for agentic reasoning for two other tasks:

1. RCA analysis for ML engineer: given a bunch of logs, identify the root cause
2. ESG Intelligence: document scoring and auditing

## Tech Stack

- **Ollama + Llama 3.1** (local inference)
- **FastAPI** (async LLM server)
- **VectorDB-based retrieval pipeline**
- **Streaming evaluation + canary testing**
- **Custom monitoring layer**
- **Streamlit operations dashboard**
- **Dockerized deployment**
- **Simulated autoscaling**
- 
This is **Task 1** of a 3-part AI project:
1. **Root Cause Analysis (SciRCA) â€” Completed**  
2. **ESG Intelligence (GreenDocs) â€” Document parsing, ESG scoring, LLM-based auditing**  
3. **AI4Science Reasoning Module â€” Model-driven scientific insight & anomaly interpretation**

---


## ğŸ— Architecture (High-Level)

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Client App       â”‚
                â”‚(Streamlit Dashboard)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚  HTTP
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    FastAPI Server   â”‚
                â”‚ - RCA Endpoint      â”‚
                â”‚ - Monitoring Layer  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Calls Agent
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     RCA Agent      â”‚
                â”‚ - Tool calls       â”‚
                â”‚ - Multi-step plan  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ LLM Chat
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Llama 3.1 (8B)    â”‚
                â”‚     via Ollama      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Tech Stack 
### **LLM / Agentic AI**
- Tool-using LLM agent (multi-step reasoning)
- Local inference via Ollama  
- Chat + tool call parsing  
- RAG pipeline integration  

### **LLMOps**
- FastAPI async server  
- Monitoring:  
  - request count  
  - latency  
  - error rate  
- Canary evaluation  
- Model registry  

### **Optimization**
- Quantized Llama models  
- Async batching of tool calls  
- Local GPU/Metal acceleration  

### **Visualization & Ops**
- Real-time dashboard (Streamlit)  
- Logs viewer  
- Inference tester  
- Latency charts  

### RAGs


### Reasoning
TODO

---


## ğŸ“¦ Directory Structure

```
scirca/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          â† RCA agent + LLM client
â”‚   â”œâ”€â”€ retriever/      â† RAG embedding + search
â”‚   â”œâ”€â”€ serve/          â† FastAPI server + monitors
â”‚   â”œâ”€â”€ utils/          â† YAML loader, logger
â”‚   â””â”€â”€ models/         â† Model registry files
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py          â† Streamlit GUI
â”‚   â””â”€â”€ components/     â† metrics, logs, registry, tester
â”‚
â”œâ”€â”€ scripts/            â† CLI scripts (eval, run agent, benchmark)
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

---

## ğŸ§ª Running Locally (macOS + Ollama)

### 1. Install Ollama
```bash
brew install ollama
```

### 2. Pull the model
```bash
ollama pull llama3.1:8b
```

### 3. Start Ollama
```bash
ollama serve
```

---

## ğŸš€ Start FastAPI Server

```bash
uvicorn src.serve.api:app --reload --port 8000
```

---

## ğŸ“Š Start Monitoring Dashboard

```bash
streamlit run dashboard/app.py
```

Open:  
ğŸ‘‰ http://localhost:8501

Dashboard Features:
- Metrics (request rate, latency, errors)
- Logs viewer (Docker/FastAPI logs)
- Model registry viewer  
- Inference runner for RCA

---

## ğŸ³ Docker Deployment

### Build
```bash
docker build -t scirca-api .
```

### Run
```bash
docker run -p 8000:8000 scirca-api
```

### Run dashboard
```bash
streamlit run dashboard/app.py
```
---

## ğŸ›  Example API Call

```bash
curl -X POST http://localhost:8000/rca   -H "Content-Type: application/json"   -d '{
        "run_summary": "Training failed with NaN loss",
        "logs": ["loss=0.5", "loss=0.7", "loss=nan"],
        "metrics": {"loss": [0.5, 0.7, "nan"]},
        "model_tag": "rca-v2"
      }'
```


## Load testing via autoscaling
```bash
python scripts/load_test.py \
    --api http://localhost:8000 \
    --concurrency 50 \
    --total 500
```
or (this second one has been tested
```bash
bash scripts/run_load_test.sh
```
---

## ğŸ§© Roadmap (Task 2 & 3)

### **Task 2 â€” ESG Intelligence (GreenDocs)**
Planned capabilities:
- ESG report ingestion  
- Compliance summarisation  
- Automated ESG scoring  
- Greenwashing detection  
- Multi-document RAG  

### **Task 3 â€” AI4Science Reasoning Module**
Planned:
- Scientific anomaly reasoning  
- Embedding-based pattern detection  
- Hypothesis generation  
- LLM-assisted interpretation of experimental results  

---

